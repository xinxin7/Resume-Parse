{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T18:45:50.650715Z",
     "start_time": "2020-06-11T18:45:50.644020Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:29:26.862573Z",
     "start_time": "2020-06-12T21:29:26.858999Z"
    }
   },
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import io\n",
    "from datetime import datetime\n",
    "from dateutil import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pdf2text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T18:59:25.654030Z",
     "start_time": "2020-06-11T18:59:25.647843Z"
    }
   },
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import io\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        # iterate over all pages of PDF document\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            # creating a resoure manager\n",
    "            resource_manager = PDFResourceManager()\n",
    "            \n",
    "            # create a file handle\n",
    "            fake_file_handle = io.StringIO()\n",
    "            \n",
    "            # creating a text converter object\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "\n",
    "            # creating a page interpreter\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "\n",
    "            # process current page\n",
    "            page_interpreter.process_page(page)\n",
    "            \n",
    "            # extract text\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "\n",
    "            # close open handles\n",
    "            converter.close()\n",
    "            fake_file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:05:15.575902Z",
     "start_time": "2020-06-12T21:05:15.160850Z"
    }
   },
   "outputs": [],
   "source": [
    "# calling above function and extracting text\n",
    "#file_path = u'/Users/eileen/DataChallenges/FORKAIA/RESUME/pyresparser-master/OmkarResume.pdf'\n",
    "# file_path = '/Users/eileen/DataChallenges/FORKAIA/RESUME/pyresparser-master/sampleCV.pdf'\n",
    "file_path = '/Users/eileen/Desktop/Xinxin Gu - Data Science - FT.pdf'\n",
    "# file_path = '/Users/eileen/DataChallenges/FORKAIA/RESUME/sample2.pdf'\n",
    "text = ''\n",
    "for page in extract_text_from_pdf(file_path):\n",
    "    text += ' ' + page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:05:17.294886Z",
     "start_time": "2020-06-12T21:05:17.285715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Xinxin Gu  \\n\\n \\n \\n \\n \\n \\n \\n\\nxinxingu7@outlook.com | (585)-754-2812 | 10011 Stonelake Blvd, Austin, TX, 78759 \\n\\nwww.linkedin.com/in/xinxin7 | https://github.com/xinxin7 \\n\\n \\nEDUCATION \\nUniversity of Rochester                                                                                                                                                   Rochester, NY                                                  \\nMaster of Data Science (30% Merit based Scholarship, GHC19’ Scholar)                                               Aug 2018 – Dec 2019 \\nCoursework: Data Mining, Machine Learning, Database System, Algorithms, Data Structure, Marketing Analysis \\nSouthwest Jiaotong University                                                                                                                                  Chengdu, China                                              \\nBachelor of Engineering in Remote Sense Science and Technology (GPA: 3.72/4.0)                              Sep 2013 – Jul 2017 \\nCoursework: Computer Vision, Spatial Data Visualization and Modeling, Digital Image Processing \\nSKILLS \\nProgramming: Python, R, MySQL, T-SQL, Spark, HTML \\nMarketing Analysis: Descriptive Analysis, Casual Analysis, Pricing Decision, A/B test, Dashboards building \\nMachine Learning: Regression, Classification, Clustering, Predictive Modeling, Dimension Reduction, Deep Learning, \\nTopic Modeling, Natural Language Processing \\nAnalytical Tools: Tableau, SPSS, AWS, Git, Packages (NumPy, Pandas, Seaborn, Matplotlib, Scikit-Learn, TensorFlow, NLTK) \\nEXPERIENCE \\nData Scientist Intern – Customer Service Group | Robert Bosch LLC.                                                           Jul 2019 – Sep 2019                                             \\nApplied text analytics and topic modeling to Helpdesk Tickets requested by customers in Bosch \\n\\n•  Designed ETL pipelines to pre-processing millions of IT tickets data from ticket systems using SQL and Python \\n•  Generate the top five topics based on NLP (word2vec, NLTK, text vectorization (BOW, TF-IDF) and LDA) \\n•  Grouped 350,000+ of IT incidents into 70 categories using K-means for better optimizing the resources \\n• \\n•  Assigned new tickets to the corresponding team automatically, reducing average resolution time by 30% \\n•  Built interactive Tickets Dashboards presenting the KPIs of tickets, quick report, Top5 ticket topics in Tableau  \\n\\nImplemented LSTM to classify the duplicated incidents and achieved 95% accuracy  \\n\\nData Analyst and Consultant Intern | Deloitte Consulting co., Ltd                                                            May 2019 – June 2019                                               \\nInvolved in updating Healthcare Information Systems for local hospitals and providing strategy support \\n•  Collaborated closely with clients who work as hospital managers to determine the requirements  \\n•  Derived the 8 KPIs relevant to the efficiency and interpreted the results to hospital managers \\n•  Built various reporting objects like Attributes, Hierarchies, Filters, Calculated fields, Parameters etc., in Tableau \\n•  Designed and implemented A/B tests to optimize the EMR system, reducing the patients wait times by 15% \\n\\nData Scientist Intern | Origent Data Sciences, Inc.                                                                  Feb 2019 – May 2019                                               \\nConducted predictive models to optimize ALS disease progression rates \\n\\n•  Created end to end ETL pipelines and performed data exploratory using Python  \\n•  Applied Random Forest, XGBoost, Light GBM and ensemble method to predict the rates of ALS patients \\n•  Achieved better predictions RMSE= 4.332 and R2 = 0.712 which improved by 5% compared to the previous work \\n•  Built interactive package using Python to evaluate the performance of models automatically \\n\\nResearch Assistant | Key Laboratory of Earth Surface Process                                                                     Jul 2017 – May 2018                                               \\nValidated a weighted moving average method in Time Series of wheat-growth \\n\\n•  Preprocessed multispectral spatial image data at a resolution of 30m and 16 days for past 30 years \\n•  Built time series of wheat-growth by an improved weighted moving average method (Weighted MA) \\n•  Developed sliding window model to forecast future temporal trends and detected the interest point \\n\\nPROJECTS \\nLaboratory Mouse Colony Management Database                                                                                       Oct 2018 – Dec 2018 \\n\\n•  Designed and conducted the relational database for the lab at Medical Center of University of Rochester \\n\\nMovie Recommendation System                                                                                                                      Oct 2018 – Nov 2018 \\n\\nImplemented Association Rule Mining (Apriori, FP-growth), user-based/item-based Collaborative Filtering \\n\\nShort period PM2.5 Prediction based on Multivariate Linear Regression Model                                                        Jul 2018 \\n\\n• \\n\\n•  Published in PLoS one (SCI), 2nd author, https://doi.org/10.1371/journal.pone.0201011  \\n\\n \\n\\n \\n\\n\\x0c'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:05:22.507731Z",
     "start_time": "2020-06-12T21:05:21.121984Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    # First name and Last name are always Proper Nouns\n",
    "#    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN', 'OP': '?'}, {'POS': 'PROPN'}]\n",
    "    matcher.add('NAME', None, pattern)\n",
    "    matches = matcher(nlp_text)\n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        return span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:05:22.738787Z",
     "start_time": "2020-06-12T21:05:22.590098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xinxin Gu'"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = extract_name(text)\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract phone numer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:05:24.868379Z",
     "start_time": "2020-06-12T21:05:24.863991Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:05:25.524442Z",
     "start_time": "2020-06-12T21:05:25.517742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5857542812'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_mobile_number(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:05:29.273898Z",
     "start_time": "2020-06-12T21:05:29.268765Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:05:29.886468Z",
     "start_time": "2020-06-12T21:05:29.880412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xinxingu7@outlook.com'"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_email(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:05:35.647228Z",
     "start_time": "2020-06-12T21:05:35.120466Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII', 'MASTER', 'BACHELOR'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:05:37.421980Z",
     "start_time": "2020-06-12T21:05:37.285785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Master', '2018'), ('Bachelor', '2013')]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_education(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.  extract sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T23:09:08.299551Z",
     "start_time": "2020-06-12T23:09:08.289681Z"
    }
   },
   "outputs": [],
   "source": [
    "RESUME_SECTIONS_GRAD = [\n",
    "                    'accomplishments',\n",
    "                    'experience',\n",
    "                    'education',\n",
    "                    'interests',\n",
    "                    'projects',\n",
    "                    'professional experience',\n",
    "                    'publications',\n",
    "                    'skills',\n",
    "                    'certifications',\n",
    "                    'objective',\n",
    "                    'career objective',\n",
    "                    'summary',\n",
    "                    'leadership',\n",
    "                    'college name'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T23:09:09.447943Z",
     "start_time": "2020-06-12T23:09:09.438384Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_entity_sections_grad(text):\n",
    "    '''\n",
    "    Helper function to extract all the raw text from sections of\n",
    "    resume specifically for graduates and undergraduates\n",
    "\n",
    "    :param text: Raw text of resume\n",
    "    :return: dictionary of entities\n",
    "    '''\n",
    "    text_split = [i.strip() for i in text.split('\\n')]\n",
    "    # sections_in_resume = [i for i in text_split if i.lower() in sections]\n",
    "    entities = {}\n",
    "    key = False\n",
    "    for phrase in text_split:\n",
    "        if len(phrase) == 1:\n",
    "            p_key = phrase\n",
    "        else:\n",
    "            p_key = set(phrase.lower().split()) & set(RESUME_SECTIONS_GRAD)\n",
    "        try:\n",
    "            p_key = list(p_key)[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        if p_key in RESUME_SECTIONS_GRAD:\n",
    "            entities[p_key] = []\n",
    "            key = p_key\n",
    "        elif key and phrase.strip():\n",
    "            entities[key].append(phrase)\n",
    "\n",
    "    # entity_key = False\n",
    "    # for entity in entities.keys():\n",
    "    #     sub_entities = {}\n",
    "    #     for entry in entities[entity]:\n",
    "    #         if u'\\u2022' not in entry:\n",
    "    #             sub_entities[entry] = []\n",
    "    #             entity_key = entry\n",
    "    #         elif entity_key:\n",
    "    #             sub_entities[entity_key].append(entry)\n",
    "    #     entities[entity] = sub_entities\n",
    "\n",
    "    # pprint.pprint(entities)\n",
    "\n",
    "    # make entities that are not found None\n",
    "    # for entity in cs.RESUME_SECTIONS:\n",
    "    #     if entity not in entities.keys():\n",
    "    #         entities[entity] = None\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T23:09:10.993833Z",
     "start_time": "2020-06-12T23:09:10.990334Z"
    }
   },
   "outputs": [],
   "source": [
    "sections = extract_entity_sections_grad(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T23:09:12.033330Z",
     "start_time": "2020-06-12T23:09:12.026558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education ['University of Rochester                                                                                                                                                   Rochester, NY', 'Master of Data Science (30% Merit based Scholarship, GHC19’ Scholar)                                               Aug 2018 – Dec 2019', 'Coursework: Data Mining, Machine Learning, Database System, Algorithms, Data Structure, Marketing Analysis', 'Southwest Jiaotong University                                                                                                                                  Chengdu, China', 'Bachelor of Engineering in Remote Sense Science and Technology (GPA: 3.72/4.0)                              Sep 2013 – Jul 2017', 'Coursework: Computer Vision, Spatial Data Visualization and Modeling, Digital Image Processing']\n",
      "*************************\n",
      "skills ['Programming: Python, R, MySQL, T-SQL, Spark, HTML', 'Marketing Analysis: Descriptive Analysis, Casual Analysis, Pricing Decision, A/B test, Dashboards building', 'Machine Learning: Regression, Classification, Clustering, Predictive Modeling, Dimension Reduction, Deep Learning,', 'Topic Modeling, Natural Language Processing', 'Analytical Tools: Tableau, SPSS, AWS, Git, Packages (NumPy, Pandas, Seaborn, Matplotlib, Scikit-Learn, TensorFlow, NLTK)']\n",
      "*************************\n",
      "experience ['Data Scientist Intern – Customer Service Group | Robert Bosch LLC.                                                           Jul 2019 – Sep 2019', 'Applied text analytics and topic modeling to Helpdesk Tickets requested by customers in Bosch', '•  Designed ETL pipelines to pre-processing millions of IT tickets data from ticket systems using SQL and Python', '•  Generate the top five topics based on NLP (word2vec, NLTK, text vectorization (BOW, TF-IDF) and LDA)', '•  Grouped 350,000+ of IT incidents into 70 categories using K-means for better optimizing the resources', '•', '•  Assigned new tickets to the corresponding team automatically, reducing average resolution time by 30%', '•  Built interactive Tickets Dashboards presenting the KPIs of tickets, quick report, Top5 ticket topics in Tableau', 'Implemented LSTM to classify the duplicated incidents and achieved 95% accuracy', 'Data Analyst and Consultant Intern | Deloitte Consulting co., Ltd                                                            May 2019 – June 2019', 'Involved in updating Healthcare Information Systems for local hospitals and providing strategy support', '•  Collaborated closely with clients who work as hospital managers to determine the requirements', '•  Derived the 8 KPIs relevant to the efficiency and interpreted the results to hospital managers', '•  Built various reporting objects like Attributes, Hierarchies, Filters, Calculated fields, Parameters etc., in Tableau', '•  Designed and implemented A/B tests to optimize the EMR system, reducing the patients wait times by 15%', 'Data Scientist Intern | Origent Data Sciences, Inc.                                                                  Feb 2019 – May 2019', 'Conducted predictive models to optimize ALS disease progression rates', '•  Created end to end ETL pipelines and performed data exploratory using Python', '•  Applied Random Forest, XGBoost, Light GBM and ensemble method to predict the rates of ALS patients', '•  Achieved better predictions RMSE= 4.332 and R2 = 0.712 which improved by 5% compared to the previous work', '•  Built interactive package using Python to evaluate the performance of models automatically', 'Research Assistant | Key Laboratory of Earth Surface Process                                                                     Jul 2017 – May 2018', 'Validated a weighted moving average method in Time Series of wheat-growth', '•  Preprocessed multispectral spatial image data at a resolution of 30m and 16 days for past 30 years', '•  Built time series of wheat-growth by an improved weighted moving average method (Weighted MA)', '•  Developed sliding window model to forecast future temporal trends and detected the interest point']\n",
      "*************************\n",
      "projects ['Laboratory Mouse Colony Management Database                                                                                       Oct 2018 – Dec 2018', '•  Designed and conducted the relational database for the lab at Medical Center of University of Rochester', 'Movie Recommendation System                                                                                                                      Oct 2018 – Nov 2018', 'Implemented Association Rule Mining (Apriori, FP-growth), user-based/item-based Collaborative Filtering', 'Short period PM2.5 Prediction based on Multivariate Linear Regression Model                                                        Jul 2018', '•', '•  Published in PLoS one (SCI), 2nd author, https://doi.org/10.1371/journal.pone.0201011']\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "for k, v in sections.items():\n",
    "    print(k, v)\n",
    "    print('*************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:19:59.525192Z",
     "start_time": "2020-06-12T21:19:59.518331Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_entities_wih_custom_model(custom_nlp_text):\n",
    "    '''\n",
    "    Helper function to extract different entities with custom\n",
    "    trained model using SpaCy's NER\n",
    "\n",
    "    :param custom_nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :return: dictionary of entities\n",
    "    '''\n",
    "    entities = {}\n",
    "    for ent in custom_nlp_text.ents:\n",
    "        if ent.label_ not in entities.keys():\n",
    "            entities[ent.label_] = [ent.text]\n",
    "        else:\n",
    "            entities[ent.label_].append(ent.text)\n",
    "    for key in entities.keys():\n",
    "        entities[key] = list(set(entities[key]))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:22:24.722584Z",
     "start_time": "2020-06-12T21:22:24.307555Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp_text = nlp(text)\n",
    "cus_sections = extract_entities_wih_custom_model(nlp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:22:25.064097Z",
     "start_time": "2020-06-12T21:22:25.057569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON ['Data Mining', '3.72/4.0', 'Xinxin Gu', 'Tableau', 'Robert Bosch LLC']\n",
      "*************************\n",
      "DATE ['Sep 2013', 'May 2019', '16 days', '10011', 'May 2018', 'June 2019', '78759', 'past 30 years', '2018', '2019']\n",
      "*************************\n",
      "GPE ['Python', 'Austin', 'Helpdesk Tickets', 'China', 'Built', 'Seaborn', 'Spark', 'Blvd', 'NY']\n",
      "*************************\n",
      "ORG ['Designed ETL', 'Collaborative Filtering \\n\\n', 'Tickets', 'PROJECTS', 'Healthcare Information Systems', 'TF-IDF', 'Spatial Data Visualization', 'GPA', 'Medical Center of University of Rochester', 'AWS', 'NLP', 'Achieved', 'SCI', 'T-SQL', 'ETL', 'SQL', 'Deloitte Consulting co.', 'Applied Random Forest', 'Python  \\n', 'TX', 'Parameters', 'SPSS', 'Python', 'NLTK', 'Created', 'TensorFlow', 'EMR', 'LDA', 'BOW', 'ALS', 'Bosch', 'University of Rochester                                                                                                                                                   ', 'Tableau', 'XGBoost']\n",
      "*************************\n",
      "PERCENT ['95%', '15%', '5%', '30%']\n",
      "*************************\n",
      "WORK_OF_ART ['Topic Modeling, Natural Language Processing']\n",
      "*************************\n",
      "NORP ['Matplotlib']\n",
      "*************************\n",
      "CARDINAL ['350,000', '0.712', 'five', '30', 'millions', '70', '8']\n",
      "*************************\n",
      "PRODUCT ['RMSE= 4.332', 'R2']\n",
      "*************************\n",
      "ORDINAL ['2nd']\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "for k, v in cus_sections.items():\n",
    "    print(k, v)\n",
    "    print('*************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. experience months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:26:01.564759Z",
     "start_time": "2020-06-12T21:26:01.561952Z"
    }
   },
   "outputs": [],
   "source": [
    "experience_list = sections['experience']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:29:32.726472Z",
     "start_time": "2020-06-12T21:29:32.718608Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_number_of_months_from_dates(date1, date2):\n",
    "    '''\n",
    "    Helper function to extract total months of experience from a resume\n",
    "\n",
    "    :param date1: Starting date\n",
    "    :param date2: Ending date\n",
    "    :return: months of experience from date1 to date2\n",
    "    '''\n",
    "    if date2.lower() == 'present':\n",
    "        date2 = datetime.now().strftime('%b %Y')\n",
    "    try:\n",
    "        if len(date1.split()[0]) > 3:\n",
    "            date1 = date1.split()\n",
    "            date1 = date1[0][:3] + ' ' + date1[1]\n",
    "        if len(date2.split()[0]) > 3:\n",
    "            date2 = date2.split()\n",
    "            date2 = date2[0][:3] + ' ' + date2[1]\n",
    "    except IndexError:\n",
    "        return 0\n",
    "    try:\n",
    "        date1 = datetime.strptime(str(date1), '%b %Y')\n",
    "        date2 = datetime.strptime(str(date2), '%b %Y')\n",
    "        months_of_experience = relativedelta.relativedelta(date2, date1)\n",
    "        months_of_experience = (months_of_experience.years\n",
    "                                * 12 + months_of_experience.months)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    return months_of_experience\n",
    "\n",
    "def get_total_experience(experience_list):\n",
    "    '''\n",
    "    Wrapper function to extract total months of experience from a resume\n",
    "\n",
    "    :param experience_list: list of experience text extracted\n",
    "    :return: total months of experience\n",
    "    '''\n",
    "    exp_ = []\n",
    "    for line in experience_list:\n",
    "        experience = re.search(\n",
    "            r'(?P<fmonth>\\w+.\\d+)\\s*(\\D|to)\\s*(?P<smonth>\\w+.\\d+|present)',\n",
    "            line,\n",
    "            re.I\n",
    "        )\n",
    "        if experience:\n",
    "            exp_.append(experience.groups())\n",
    "    total_exp = sum(\n",
    "        [get_number_of_months_from_dates(i[0], i[2]) for i in exp_]\n",
    "    )\n",
    "    total_experience_in_months = total_exp\n",
    "    return total_experience_in_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T21:29:32.737990Z",
     "start_time": "2020-06-12T21:29:32.729058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_total_experience(experience_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
